{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('../speeches_millercenter_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Summary</th>\n",
       "      <th>President</th>\n",
       "      <th>Source</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34th time speak oval office year soon time wan...</td>\n",
       "      <td>In this broadcast from the Oval Office, Presid...</td>\n",
       "      <td>Ronald Reagan</td>\n",
       "      <td>National Archives</td>\n",
       "      <td>January 11, 1989</td>\n",
       "      <td>https://millercenter.org/the-presidency/presid...</td>\n",
       "      <td>3289</td>\n",
       "      <td>Republican</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Transcript  \\\n",
       "0  34th time speak oval office year soon time wan...   \n",
       "\n",
       "                                             Summary      President  \\\n",
       "0  In this broadcast from the Oval Office, Presid...  Ronald Reagan   \n",
       "\n",
       "              Source              Date  \\\n",
       "0  National Archives  January 11, 1989   \n",
       "\n",
       "                                                 URL  Word Count Affiliation  \\\n",
       "0  https://millercenter.org/the-presidency/presid...        3289  Republican   \n",
       "\n",
       "   Label  \n",
       "0      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((312,), (79,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names = train_test_split(text_data['Transcript'],\n",
    "                                         np.array(text_data['Label']),\n",
    "                                         np.array(text_data['Affiliation']),\n",
    "                                         test_size=0.2, random_state=42)\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130,), (44,), (79,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation and train split\n",
    "train_corpus, validation_corpus, train_label_nums, validation_label_nums, train_label_names, validation_label_names = train_test_split(train_corpus,\n",
    "                                         train_label_nums,\n",
    "                                         train_label_names,\n",
    "                                         test_size=0.25, random_state=42)\n",
    "train_corpus.shape, validation_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training speeches\n",
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(train_corpus)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('esteem', 11550) ('<PAD>', 0) 1\n"
     ]
    }
   ],
   "source": [
    "print(max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n",
    "      min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n",
    "      t.word_index['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(train_corpus)\n",
    "val_sequences = t.texts_to_sequences(validation_corpus)\n",
    "test_sequences = t.texts_to_sequences(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=11551\n",
      "Number of Documents=130\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the below do? Limit the vector length? And why do we need it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130, 500), (44, 500), (79, 500))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# pad dataset to a maximum review length in words\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val = tf.keras.preprocessing.sequence.pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300\n",
    "EPOCHS=100\n",
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = t.word_index\n",
    "# Word vectors downloaded from https://fasttext.cc/docs/en/english-vectors.html\n",
    "FASTTEXT_INIT_EMBEDDINGS_FILE = '../wiki-news-300d-1M-subword.vec'\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(word_to_index, max_features, embedding_size, embedding_file_path):    \n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*row.split(\" \")) \n",
    "                                for row in open(embedding_file_path, encoding=\"utf8\", errors='ignore') \n",
    "                                    if len(row)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_features: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does exactly the below? It uses the embeddings to create the corpus vector representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_capstone/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11551, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_embeddings = load_pretrained_embeddings(word_to_index=word2idx, \n",
    "                                           max_features=VOCAB_SIZE, \n",
    "                                           embedding_size=EMBED_SIZE, \n",
    "                                           embedding_file_path=FASTTEXT_INIT_EMBEDDINGS_FILE)\n",
    "ft_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 300)          3465300   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 256)          384256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 20, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 20, 64)            41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,186,389\n",
      "Trainable params: 4,186,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_SIZE,\n",
    "                                    weights=[ft_embeddings],\n",
    "                                    trainable=True,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=256, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=5))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=5))\n",
    "\n",
    "model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling1D(pool_size=5))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the validation dataset used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 149ms/step - loss: 0.6875 - accuracy: 0.6308 - val_loss: 0.6813 - val_accuracy: 0.5682\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6570 - accuracy: 0.6462 - val_loss: 0.6997 - val_accuracy: 0.5682\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6540 - accuracy: 0.6462 - val_loss: 0.7038 - val_accuracy: 0.5682\n",
      "Epoch 4/100\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6469 - accuracy: 0.6406Restoring model weights from the end of the best epoch.\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6447 - accuracy: 0.6462 - val_loss: 0.6877 - val_accuracy: 0.5682\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3ef07450>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                      patience=3,\n",
    "                                      restore_best_weights=True,\n",
    "                                      verbose=1)\n",
    "\n",
    "model.fit(X_train, train_label_nums, \n",
    "          validation_data=(X_val, validation_label_nums),\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          shuffle=True,\n",
    "          callbacks=[es],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-e42da4db2cc2>:2: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "Accuracy: 50.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        39\n",
      "           1       0.51      1.00      0.67        40\n",
      "\n",
      "    accuracy                           0.51        79\n",
      "   macro avg       0.25      0.50      0.34        79\n",
      "weighted avg       0.26      0.51      0.34        79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_capstone/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  0  39\n",
       "1  0  40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "predictions = model.predict_classes(X_test, batch_size=2048, verbose=0).ravel()\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(test_label_nums.astype(int), predictions)*100))\n",
    "print(classification_report(test_label_nums, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_label_nums, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
