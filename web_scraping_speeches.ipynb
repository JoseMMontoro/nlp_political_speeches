{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First site to parse: https://www.americanrhetoric.com/speeches/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_file(python_list, path):\n",
    "    with open(path, 'w') as f:\n",
    "        f.write('\\n'.join(python_list))\n",
    "        \n",
    "def remove_dupes_file(file_path, new_file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        unique_lines = set(lines)\n",
    "    with open(new_file_path, 'w') as g:\n",
    "        g.write('\\n'.join(unique_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    '''Extracts all links on a URL'''\n",
    "    html_page = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669 links in section 'a-f'\n",
      "554 links in section 'g-l'\n",
      "560 links in section 'm-r'\n",
      "509 links in section 's-z'\n",
      "Total links: 2245\n"
     ]
    }
   ],
   "source": [
    "# From the americanrethoric.com website\n",
    "rethoric_links = []\n",
    "sections = ['a-f', 'g-l', 'm-r', 's-z']\n",
    "for section in sections:\n",
    "    url = 'https://www.americanrhetoric.com/speechbank{}.htm'.format(section)\n",
    "    links = get_links(url)\n",
    "    print(\"{} links in section '{}'\".format(len(links), section))\n",
    "    [rethoric_links.append(link) for link in links if link is not None]\n",
    "print(\"Total links: {}\".format(len(rethoric_links)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_file(rethoric_links, \"american_rethoric_links.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dupes_file('list_of_presidents.txt', 'unique_list_of_presidents.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_president_names(file_path):\n",
    "    '''Gets individual names and last names as list from text file'''\n",
    "    with open(file_path, 'r') as f:\n",
    "        names = f.read().splitlines()\n",
    "        names_unique = set(names)\n",
    "        print(\"Total politician full names: {}. Unique: {}\".format(len(names), len(names_unique)))\n",
    "        split_names = []\n",
    "        _ = [split_names.extend(name.split(' ')) for name in names_unique]\n",
    "        president_names = [item for item in split_names if len(item) > 2]\n",
    "        print(\"Total splitted names to search: {}\".format(len(president_names)))\n",
    "        return president_names\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total politician full names: 14. Unique: 14\n",
      "Total splitted names to search: 27\n",
      "Total politician full names: 14. Unique: 14\n",
      "Total splitted names to search: 28\n"
     ]
    }
   ],
   "source": [
    "democratic_names = extract_president_names('democratic_politicians.txt')\n",
    "republican_names = extract_president_names('republican_politicians.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matching_urls_amrethoric(politician_names, links):\n",
    "    '''Extract links with politician name in them, from the list\n",
    "    of links in the americanrethoric site. Returns a dictionary \n",
    "    with the names as keys, and a flat list with all links'''\n",
    "    presidential_urls = dict()\n",
    "    for name in politician_names:\n",
    "        # Extract all links that aren't None and that match the lowercased name and that end in htm\n",
    "        matching = [link for link in rethoric_links if link is not None and name.lower() in link.lower() and link[-4:]== '.htm']\n",
    "        presidential_urls[name] = matching\n",
    "    list_of_links = []\n",
    "    _ = [list_of_links.extend(val) for val in presidential_urls.values()]\n",
    "    unique_links = set(list_of_links)\n",
    "    print(\"Unique links: {}, Matching links: {}, Total links: {}\".format(\n",
    "        len(unique_links), len(list_of_links), len(links)))\n",
    "    return unique_links, list_of_links, presidential_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique links: 217, Matching links: 379, Total links: 2245\n",
      "Unique links: 243, Matching links: 445, Total links: 2245\n"
     ]
    }
   ],
   "source": [
    "dem_links_list = extract_matching_urls_amrethoric(democratic_names, rethoric_links)[0]\n",
    "rep_links_list = extract_matching_urls_amrethoric(republican_names, rethoric_links)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speeches/johnmccainlibertycommencement.htm',\n",
       " 'speeches/johnmccainmunichsecurityconference2017.htm',\n",
       " 'speeches/philknightjoepaternomemorial.htm',\n",
       " 'speeches/wjclintonmemphis.htm',\n",
       " 'speeches/brianmulroneyeulogyforgeorgehwbush.htm']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dem_links_list)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_one_speech(speech_paragraphs):\n",
    "    '''Extracts and processes the text from a single speech/page'''\n",
    "    speech_joined = []\n",
    "    for paragraph in speech_paragraphs:\n",
    "        clean_paragraph = paragraph.get_text(strip=True).replace('\\r\\n\\t\\t\\t', '')\n",
    "        speech_joined.append(clean_paragraph)\n",
    "    speech_joined = ' '.join(speech_joined)\n",
    "    return speech_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_links_list(link_list):\n",
    "    '''Takes a list of URLS with speeches and returns the clean speeches (from americanrethoric)'''\n",
    "    root_url = 'https://www.americanrhetoric.com/'\n",
    "    all_speeches = []\n",
    "    for link in tqdm(link_list):\n",
    "        url = root_url + link\n",
    "        html_page = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_page, from_encoding='cp1252')\n",
    "        paragraphs = soup.find_all('p', style=\"line-height: 130%\")\n",
    "        speech_clean = join_one_speech(paragraphs)\n",
    "        if speech_clean:\n",
    "            all_speeches.append(speech_clean)\n",
    "    time.sleep(3)\n",
    "    print(\"Finished list of links. {} speeches saved of {} links processed\".format(\n",
    "                    len(all_speeches), len(link_list)))\n",
    "    return all_speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217/217 [00:50<00:00,  4.31it/s]\n",
      "  0%|          | 0/243 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished list of links. 106 speeches saved of 217 links processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 243/243 [00:59<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished list of links. 121 speeches saved of 243 links processed\n"
     ]
    }
   ],
   "source": [
    "americanrethoric_democrat = process_links_list(dem_links_list)\n",
    "americanrethoric_republican = process_links_list(rep_links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic speeches number of words: 280,763\n",
      "Democratic speeches number of words: 303,904\n"
     ]
    }
   ],
   "source": [
    "print('Democratic speeches number of words: {:,}'.format(len('\\n'.join(americanrethoric_democrat).split())))\n",
    "print('Democratic speeches number of words: {:,}'.format(len('\\n'.join(americanrethoric_republican).split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "\n",
    "- We should show a graph with how much data is from each individual person. Ex: is likely that there will be more Obama speeches than anybody eslse, so classifying as democrat will be skewed by that - almost like classified as Obama-like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "1. Which format should I save the data in? DataFrame? Which structure - columns and index names?\n",
    "2. Issue with encoding - seems to be not using the right encoding 'cp1252', but also seems like the apostrophes are escaped in the final string?\n",
    "- Difference between bytes and string types in Python 3. \n",
    "- How to use encode and decode\n",
    "3. How to calculate how much data we have? Number of speeches is misleading as they might be very different in length. Number of characters? Number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is attempt to parse the rev.com site. Will try further ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/josemontoro/data/personal/springboard/repos/PySitemap')\n",
    "import crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.rev.com/blog/transcripts/'\n",
    "\n",
    "crawl = crawler.Crawler(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = crawl.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is attempt to parse the Millcenter.org site. Will try further ahead.\n",
    "\n",
    "president_ids = [39, 43, 8396, 40, 35, 42, 38, 34, 37,41]\n",
    "presidents = ['Reagan', 'Obama', 'Trump', 'H.W.Bush', 'Lyndon', 'Bush', 'Carter', 'Kennedy', 'Ford', 'Clinton']\n",
    "\n",
    "# URL structure = https://millercenter.org/the-presidency/presidential-speeches?field_president_target_id[43]=43\n",
    "\n",
    "\n",
    "for president_id in president_ids:\n",
    "    url = 'https://millercenter.org/the-presidency/presidential-speeches?field_president_target_id[{}]={}'.format(\n",
    "    president_id, president_id)\n",
    "    links = get_links(url)\n",
    "    break\n",
    "links"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
